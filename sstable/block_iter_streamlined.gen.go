// Code generated by block-iter-codegen; DO NOT EDIT.

package sstable

import (
	"bytes"
	"encoding/binary"
	"unsafe"
	"slices"

	"github.com/cockroachdb/errors"
	"github.com/cockroachdb/pebble/internal/base"
	"github.com/cockroachdb/pebble/internal/invariants"
	"github.com/cockroachdb/pebble/internal/manual"
)

func (i *blockIter) streamlinedReadEntry() {
	ptr := unsafe.Pointer(uintptr(i.ptr) + uintptr(i.offset))

	var shared uint32
	if a := *((*uint8)(ptr)); a < 128 {
		shared = uint32(a)
		ptr = unsafe.Pointer(uintptr(ptr) + 1)
	} else if a, b := a&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 1))); b < 128 {
		shared = uint32(b)<<7 | uint32(a)
		ptr = unsafe.Pointer(uintptr(ptr) + 2)
	} else if b, c := b&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 2))); c < 128 {
		shared = uint32(c)<<14 | uint32(b)<<7 | uint32(a)
		ptr = unsafe.Pointer(uintptr(ptr) + 3)
	} else if c, d := c&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 3))); d < 128 {
		shared = uint32(d)<<21 | uint32(c)<<14 | uint32(b)<<7 | uint32(a)
		ptr = unsafe.Pointer(uintptr(ptr) + 4)
	} else {
		d, e := d&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 4)))
		shared = uint32(e)<<28 | uint32(d)<<21 | uint32(c)<<14 | uint32(b)<<7 | uint32(a)
		ptr = unsafe.Pointer(uintptr(ptr) + 5)
	}

	var unshared uint32
	if a := *((*uint8)(ptr)); a < 128 {
		unshared = uint32(a)
		ptr = unsafe.Pointer(uintptr(ptr) + 1)
	} else if a, b := a&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 1))); b < 128 {
		unshared = uint32(b)<<7 | uint32(a)
		ptr = unsafe.Pointer(uintptr(ptr) + 2)
	} else if b, c := b&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 2))); c < 128 {
		unshared = uint32(c)<<14 | uint32(b)<<7 | uint32(a)
		ptr = unsafe.Pointer(uintptr(ptr) + 3)
	} else if c, d := c&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 3))); d < 128 {
		unshared = uint32(d)<<21 | uint32(c)<<14 | uint32(b)<<7 | uint32(a)
		ptr = unsafe.Pointer(uintptr(ptr) + 4)
	} else {
		d, e := d&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 4)))
		unshared = uint32(e)<<28 | uint32(d)<<21 | uint32(c)<<14 | uint32(b)<<7 | uint32(a)
		ptr = unsafe.Pointer(uintptr(ptr) + 5)
	}

	var value uint32
	if a := *((*uint8)(ptr)); a < 128 {
		value = uint32(a)
		ptr = unsafe.Pointer(uintptr(ptr) + 1)
	} else if a, b := a&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 1))); b < 128 {
		value = uint32(b)<<7 | uint32(a)
		ptr = unsafe.Pointer(uintptr(ptr) + 2)
	} else if b, c := b&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 2))); c < 128 {
		value = uint32(c)<<14 | uint32(b)<<7 | uint32(a)
		ptr = unsafe.Pointer(uintptr(ptr) + 3)
	} else if c, d := c&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 3))); d < 128 {
		value = uint32(d)<<21 | uint32(c)<<14 | uint32(b)<<7 | uint32(a)
		ptr = unsafe.Pointer(uintptr(ptr) + 4)
	} else {
		d, e := d&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 4)))
		value = uint32(e)<<28 | uint32(d)<<21 | uint32(c)<<14 | uint32(b)<<7 | uint32(a)
		ptr = unsafe.Pointer(uintptr(ptr) + 5)
	}
	if !true {
		shared += uint32(len(i.transforms.SyntheticPrefix))
	}
	unsharedKey := getBytes(ptr, int(unshared))

	i.fullKey = append(i.fullKey[:shared], unsharedKey...)
	if shared == 0 {

		i.key = unsharedKey
	} else {
		i.key = i.fullKey
	}
	ptr = unsafe.Pointer(uintptr(ptr) + uintptr(unshared))
	i.val = getBytes(ptr, int(value))
	i.nextOffset = int32(uintptr(ptr)-uintptr(i.ptr)) + int32(value)
}

func (i *blockIter) streamlinedReadFirstKey() error {
	ptr := i.ptr

	if shared := *((*uint8)(ptr)); shared == 0 {
		ptr = unsafe.Pointer(uintptr(ptr) + 1)
	} else {

		panic("first key in block must have zero shared length")
	}

	var unshared uint32
	if a := *((*uint8)(ptr)); a < 128 {
		unshared = uint32(a)
		ptr = unsafe.Pointer(uintptr(ptr) + 1)
	} else if a, b := a&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 1))); b < 128 {
		unshared = uint32(b)<<7 | uint32(a)
		ptr = unsafe.Pointer(uintptr(ptr) + 2)
	} else if b, c := b&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 2))); c < 128 {
		unshared = uint32(c)<<14 | uint32(b)<<7 | uint32(a)
		ptr = unsafe.Pointer(uintptr(ptr) + 3)
	} else if c, d := c&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 3))); d < 128 {
		unshared = uint32(d)<<21 | uint32(c)<<14 | uint32(b)<<7 | uint32(a)
		ptr = unsafe.Pointer(uintptr(ptr) + 4)
	} else {
		d, e := d&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 4)))
		unshared = uint32(e)<<28 | uint32(d)<<21 | uint32(c)<<14 | uint32(b)<<7 | uint32(a)
		ptr = unsafe.Pointer(uintptr(ptr) + 5)
	}

	if a := *((*uint8)(ptr)); a < 128 {
		ptr = unsafe.Pointer(uintptr(ptr) + 1)
	} else if a := *((*uint8)(unsafe.Pointer(uintptr(ptr) + 1))); a < 128 {
		ptr = unsafe.Pointer(uintptr(ptr) + 2)
	} else if a := *((*uint8)(unsafe.Pointer(uintptr(ptr) + 2))); a < 128 {
		ptr = unsafe.Pointer(uintptr(ptr) + 3)
	} else if a := *((*uint8)(unsafe.Pointer(uintptr(ptr) + 3))); a < 128 {
		ptr = unsafe.Pointer(uintptr(ptr) + 4)
	} else {
		ptr = unsafe.Pointer(uintptr(ptr) + 5)
	}

	firstKey := getBytes(ptr, int(unshared))

	if n := len(firstKey) - 8; n >= 0 {
		i.firstUserKey = firstKey[:n:n]
	} else {
		i.firstUserKey = nil
		return base.CorruptionErrorf("pebble/table: invalid firstKey in block")
	}
	if !true && i.transforms.SyntheticPrefix != nil {
		i.firstUserKeyWithPrefixBuf = slices.Grow(i.firstUserKeyWithPrefixBuf[:0], len(i.transforms.SyntheticPrefix)+len(i.firstUserKey))
		i.firstUserKeyWithPrefixBuf = append(i.firstUserKeyWithPrefixBuf, i.transforms.SyntheticPrefix...)
		i.firstUserKeyWithPrefixBuf = append(i.firstUserKeyWithPrefixBuf, i.firstUserKey...)
		i.firstUserKey = i.firstUserKeyWithPrefixBuf
	}
	return nil
}

// SeekGE implements internalIterator.SeekGE, as documented in the pebble
// package.
func (i *blockIter) StreamlinedSeekGE(key []byte, flags base.SeekGEFlags) (*InternalKey, base.LazyValue) {
	if invariants.Enabled && i.isDataInvalidated() {
		panic(errors.AssertionFailedf("invalidated blockIter used"))
	}
	searchKey := key
	if !true && i.transforms.SyntheticPrefix != nil {

		if !bytes.HasPrefix(key, i.transforms.SyntheticPrefix) {
			if i.cmp(i.firstUserKey, key) >= 0 {
				return i.StreamlinedFirst()
			}

			i.offset = i.restarts
			i.nextOffset = i.restarts
			return nil, base.LazyValue{}
		}
		searchKey = key[len(i.transforms.SyntheticPrefix):]
	}

	i.clearCache()

	i.offset = 0
	var index int32

	{

		upper := i.numRestarts
		for index < upper {
			h := int32(uint(index+upper) >> 1)

			offset := decodeRestart(i.data[i.restarts+4*h:])

			ptr := unsafe.Pointer(uintptr(i.ptr) + uintptr(offset+1))

			// Decode the key at that restart point, and compare it to the key
			// sought. See the comment in readEntry for why we manually inline the
			// varint decoding.
			var v1 uint32
			if a := *((*uint8)(ptr)); a < 128 {
				v1 = uint32(a)
				ptr = unsafe.Pointer(uintptr(ptr) + 1)
			} else if a, b := a&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 1))); b < 128 {
				v1 = uint32(b)<<7 | uint32(a)
				ptr = unsafe.Pointer(uintptr(ptr) + 2)
			} else if b, c := b&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 2))); c < 128 {
				v1 = uint32(c)<<14 | uint32(b)<<7 | uint32(a)
				ptr = unsafe.Pointer(uintptr(ptr) + 3)
			} else if c, d := c&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 3))); d < 128 {
				v1 = uint32(d)<<21 | uint32(c)<<14 | uint32(b)<<7 | uint32(a)
				ptr = unsafe.Pointer(uintptr(ptr) + 4)
			} else {
				d, e := d&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 4)))
				v1 = uint32(e)<<28 | uint32(d)<<21 | uint32(c)<<14 | uint32(b)<<7 | uint32(a)
				ptr = unsafe.Pointer(uintptr(ptr) + 5)
			}

			if *((*uint8)(ptr)) < 128 {
				ptr = unsafe.Pointer(uintptr(ptr) + 1)
			} else if *((*uint8)(unsafe.Pointer(uintptr(ptr) + 1))) < 128 {
				ptr = unsafe.Pointer(uintptr(ptr) + 2)
			} else if *((*uint8)(unsafe.Pointer(uintptr(ptr) + 2))) < 128 {
				ptr = unsafe.Pointer(uintptr(ptr) + 3)
			} else if *((*uint8)(unsafe.Pointer(uintptr(ptr) + 3))) < 128 {
				ptr = unsafe.Pointer(uintptr(ptr) + 4)
			} else {
				ptr = unsafe.Pointer(uintptr(ptr) + 5)
			}

			s := getBytes(ptr, int(v1))
			var k []byte
			if n := len(s) - 8; n >= 0 {
				k = s[:n:n]
			}

			if i.cmp(searchKey, k) > 0 {

				index = h + 1
			} else {

				upper = h
			}
		}

	}

	if index > 0 {
		i.offset = decodeRestart(i.data[i.restarts+4*(index-1):])
	}
	i.streamlinedReadEntry()
	hiddenPoint := i.decodeInternalKey(i.key)

	if !i.valid() {
		return nil, base.LazyValue{}
	}

	i.maybeReplaceSuffix(true)

	if !hiddenPoint && i.cmp(i.ikey.UserKey, key) >= 0 {

		if !i.lazyValueHandling.hasValuePrefix ||
			base.TrailerKind(i.ikey.Trailer) != InternalKeyKindSet {
			i.lazyValue = base.MakeInPlaceValue(i.val)
		} else if i.lazyValueHandling.vbr == nil || !isValueHandle(valuePrefix(i.val[0])) {
			i.lazyValue = base.MakeInPlaceValue(i.val[1:])
		} else {
			i.lazyValue = i.lazyValueHandling.vbr.getLazyValueForPrefixAndValueHandle(i.val)
		}
		return &i.ikey, i.lazyValue
	}
	for i.StreamlinedNext(); i.valid(); i.StreamlinedNext() {
		if i.cmp(i.ikey.UserKey, key) >= 0 {

			return &i.ikey, i.lazyValue
		}
	}
	return nil, base.LazyValue{}
}

// SeekLT implements internalIterator.SeekLT, as documented in the pebble
// package.
func (i *blockIter) StreamlinedSeekLT(key []byte, flags base.SeekLTFlags) (*InternalKey, base.LazyValue) {
	if invariants.Enabled && i.isDataInvalidated() {
		panic(errors.AssertionFailedf("invalidated blockIter used"))
	}
	searchKey := key
	if i.transforms.SyntheticPrefix != nil {

		if !bytes.HasPrefix(key, i.transforms.SyntheticPrefix) {
			if i.cmp(i.firstUserKey, key) < 0 {
				return i.StreamlinedLast()
			}

			i.offset = -1
			i.nextOffset = 0
			return nil, base.LazyValue{}
		}
		searchKey = key[len(i.transforms.SyntheticPrefix):]
	}

	i.clearCache()

	i.offset = 0
	var index int32

	{

		upper := i.numRestarts
		for index < upper {
			h := int32(uint(index+upper) >> 1)

			offset := decodeRestart(i.data[i.restarts+4*h:])

			ptr := unsafe.Pointer(uintptr(i.ptr) + uintptr(offset+1))

			// Decode the key at that restart point, and compare it to the key
			// sought. See the comment in readEntry for why we manually inline the
			// varint decoding.
			var v1 uint32
			if a := *((*uint8)(ptr)); a < 128 {
				v1 = uint32(a)
				ptr = unsafe.Pointer(uintptr(ptr) + 1)
			} else if a, b := a&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 1))); b < 128 {
				v1 = uint32(b)<<7 | uint32(a)
				ptr = unsafe.Pointer(uintptr(ptr) + 2)
			} else if b, c := b&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 2))); c < 128 {
				v1 = uint32(c)<<14 | uint32(b)<<7 | uint32(a)
				ptr = unsafe.Pointer(uintptr(ptr) + 3)
			} else if c, d := c&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 3))); d < 128 {
				v1 = uint32(d)<<21 | uint32(c)<<14 | uint32(b)<<7 | uint32(a)
				ptr = unsafe.Pointer(uintptr(ptr) + 4)
			} else {
				d, e := d&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 4)))
				v1 = uint32(e)<<28 | uint32(d)<<21 | uint32(c)<<14 | uint32(b)<<7 | uint32(a)
				ptr = unsafe.Pointer(uintptr(ptr) + 5)
			}

			if *((*uint8)(ptr)) < 128 {
				ptr = unsafe.Pointer(uintptr(ptr) + 1)
			} else if *((*uint8)(unsafe.Pointer(uintptr(ptr) + 1))) < 128 {
				ptr = unsafe.Pointer(uintptr(ptr) + 2)
			} else if *((*uint8)(unsafe.Pointer(uintptr(ptr) + 2))) < 128 {
				ptr = unsafe.Pointer(uintptr(ptr) + 3)
			} else if *((*uint8)(unsafe.Pointer(uintptr(ptr) + 3))) < 128 {
				ptr = unsafe.Pointer(uintptr(ptr) + 4)
			} else {
				ptr = unsafe.Pointer(uintptr(ptr) + 5)
			}

			s := getBytes(ptr, int(v1))
			var k []byte
			if n := len(s) - 8; n >= 0 {
				k = s[:n:n]
			}

			if i.cmp(searchKey, k) > 0 {

				index = h + 1
			} else {

				upper = h
			}
		}

	}

	if index == 0 {
		if i.transforms.SyntheticSuffix != nil {

			ikey, lazyVal := i.StreamlinedFirst()
			if i.cmp(ikey.UserKey, key) < 0 {
				return ikey, lazyVal
			}
		}

		i.offset = -1
		i.nextOffset = 0
		return nil, base.LazyValue{}
	}

	targetOffset := i.restarts
	i.offset = decodeRestart(i.data[i.restarts+4*(index-1):])
	if index < i.numRestarts {
		targetOffset = decodeRestart(i.data[i.restarts+4*(index):])

		if i.transforms.SyntheticSuffix != nil {

			naiveOffset := i.offset

			i.offset = targetOffset
			i.streamlinedReadEntry()
			i.decodeInternalKey(i.key)
			i.maybeReplaceSuffix(false)

			if i.cmp(i.ikey.UserKey, key) < 0 {
				i.offset = targetOffset
				if index+1 < i.numRestarts {

					targetOffset = decodeRestart(i.data[i.restarts+4*(index+1):])
				} else {
					targetOffset = i.restarts
				}
			} else {
				i.offset = naiveOffset
			}
		}
	}

	i.nextOffset = i.offset

	for {
		i.offset = i.nextOffset
		i.streamlinedReadEntry()

		hiddenPoint := i.decodeInternalKey(i.key)
		i.maybeReplaceSuffix(false)

		if i.cmp(i.ikey.UserKey, key) >= 0 {

			return i.StreamlinedPrev()
		}

		if i.nextOffset >= targetOffset {

			if hiddenPoint {
				return i.StreamlinedPrev()
			}
			break
		}
		i.cacheEntry()
	}

	if !i.valid() {
		return nil, base.LazyValue{}
	}
	if !i.lazyValueHandling.hasValuePrefix ||
		base.TrailerKind(i.ikey.Trailer) != InternalKeyKindSet {
		i.lazyValue = base.MakeInPlaceValue(i.val)
	} else if i.lazyValueHandling.vbr == nil || !isValueHandle(valuePrefix(i.val[0])) {
		i.lazyValue = base.MakeInPlaceValue(i.val[1:])
	} else {
		i.lazyValue = i.lazyValueHandling.vbr.getLazyValueForPrefixAndValueHandle(i.val)
	}
	return &i.ikey, i.lazyValue
}

// First implements internalIterator.First, as documented in the pebble
// package.
func (i *blockIter) StreamlinedFirst() (*InternalKey, base.LazyValue) {
	if invariants.Enabled && i.isDataInvalidated() {
		panic(errors.AssertionFailedf("invalidated blockIter used"))
	}

	i.offset = 0
	if !i.valid() {
		return nil, base.LazyValue{}
	}
	i.clearCache()
	i.streamlinedReadEntry()
	hiddenPoint := i.decodeInternalKey(i.key)
	if hiddenPoint {
		return i.StreamlinedNext()
	}
	i.maybeReplaceSuffix(true)
	if !i.lazyValueHandling.hasValuePrefix ||
		base.TrailerKind(i.ikey.Trailer) != InternalKeyKindSet {
		i.lazyValue = base.MakeInPlaceValue(i.val)
	} else if i.lazyValueHandling.vbr == nil || !isValueHandle(valuePrefix(i.val[0])) {
		i.lazyValue = base.MakeInPlaceValue(i.val[1:])
	} else {
		i.lazyValue = i.lazyValueHandling.vbr.getLazyValueForPrefixAndValueHandle(i.val)
	}
	return &i.ikey, i.lazyValue
}

// Last implements internalIterator.Last, as documented in the pebble package.
func (i *blockIter) StreamlinedLast() (*InternalKey, base.LazyValue) {
	if invariants.Enabled && i.isDataInvalidated() {
		panic(errors.AssertionFailedf("invalidated blockIter used"))
	}

	i.offset = decodeRestart(i.data[i.restarts+4*(i.numRestarts-1):])
	if !i.valid() {
		return nil, base.LazyValue{}
	}

	i.streamlinedReadEntry()
	i.clearCache()

	for i.nextOffset < i.restarts {
		i.cacheEntry()
		i.offset = i.nextOffset
		i.streamlinedReadEntry()
	}

	hiddenPoint := i.decodeInternalKey(i.key)
	if hiddenPoint {
		return i.StreamlinedPrev()
	}
	i.maybeReplaceSuffix(false)
	if !i.lazyValueHandling.hasValuePrefix ||
		base.TrailerKind(i.ikey.Trailer) != InternalKeyKindSet {
		i.lazyValue = base.MakeInPlaceValue(i.val)
	} else if i.lazyValueHandling.vbr == nil || !isValueHandle(valuePrefix(i.val[0])) {
		i.lazyValue = base.MakeInPlaceValue(i.val[1:])
	} else {
		i.lazyValue = i.lazyValueHandling.vbr.getLazyValueForPrefixAndValueHandle(i.val)
	}
	return &i.ikey, i.lazyValue
}

// Next implements internalIterator.Next, as documented in the pebble
// package.
func (i *blockIter) StreamlinedNext() (*InternalKey, base.LazyValue) {
	if len(i.cachedBuf) > 0 {

		i.fullKey = append(i.fullKey[:0], i.key...)
		i.clearCache()
	}

start:
	i.offset = i.nextOffset
	if !i.valid() {
		return nil, base.LazyValue{}
	}
	i.streamlinedReadEntry()

	if n := len(i.key) - 8; n >= 0 {
		trailer := binary.LittleEndian.Uint64(i.key[n:])
		hiddenPoint := i.transforms.HideObsoletePoints &&
			(trailer&trailerObsoleteBit != 0)
		i.ikey.Trailer = trailer & trailerObsoleteMask
		i.ikey.UserKey = i.key[:n:n]
		if n := i.transforms.SyntheticSeqNum; n != 0 {
			i.ikey.SetSeqNum(uint64(n))
		}
		if hiddenPoint {
			goto start
		}
		if i.transforms.SyntheticSuffix != nil {

			prefixLen := i.split(i.ikey.UserKey)
			if cap(i.ikey.UserKey) >= prefixLen+len(i.transforms.SyntheticSuffix) {
				i.ikey.UserKey = append(i.ikey.UserKey[:prefixLen], i.transforms.SyntheticSuffix...)
			} else {
				i.synthSuffixBuf = append(i.synthSuffixBuf[:0], i.ikey.UserKey[:prefixLen]...)
				i.synthSuffixBuf = append(i.synthSuffixBuf, i.transforms.SyntheticSuffix...)
				i.ikey.UserKey = i.synthSuffixBuf
			}
		}
	} else {
		i.ikey.Trailer = uint64(InternalKeyKindInvalid)
		i.ikey.UserKey = nil
	}
	if !i.lazyValueHandling.hasValuePrefix ||
		base.TrailerKind(i.ikey.Trailer) != InternalKeyKindSet {
		i.lazyValue = base.MakeInPlaceValue(i.val)
	} else if i.lazyValueHandling.vbr == nil || !isValueHandle(valuePrefix(i.val[0])) {
		i.lazyValue = base.MakeInPlaceValue(i.val[1:])
	} else {
		i.lazyValue = i.lazyValueHandling.vbr.getLazyValueForPrefixAndValueHandle(i.val)
	}
	return &i.ikey, i.lazyValue
}

// NextPrefix implements (base.InternalIterator).NextPrefix.
func (i *blockIter) StreamlinedNextPrefix(succKey []byte) (*InternalKey, base.LazyValue) {
	if i.lazyValueHandling.hasValuePrefix {
		return i.streamlinedNextPrefixV3(succKey)
	}
	const nextsBeforeSeek = 3
	k, v := i.StreamlinedNext()
	for j := 1; k != nil && i.cmp(k.UserKey, succKey) < 0; j++ {
		if j >= nextsBeforeSeek {
			return i.StreamlinedSeekGE(succKey, base.SeekGEFlagsNone)
		}
		k, v = i.StreamlinedNext()
	}
	return k, v
}

func (i *blockIter) streamlinedNextPrefixV3(succKey []byte) (*InternalKey, base.LazyValue) {
	// Doing nexts that involve a key comparison can be expensive (and the cost
	// depends on the key length), so we use the same threshold of 3 that we use
	// for TableFormatPebblev2 in blockIter.nextPrefix above. The next fast path
	// that looks at setHasSamePrefix takes ~5ns per key, which is ~150x faster
	// than doing a SeekGE within the block, so we do this 16 times
	// (~5ns*16=80ns), and then switch to looking at restarts. Doing the binary
	// search for the restart consumes > 100ns. If the number of versions is >
	// 17, we will increment nextFastCount to 17, then do a binary search, and
	// on average need to find a key between two restarts, so another 8 steps
	// corresponding to nextFastCount, for a mean total of 17 + 8 = 25 such
	// steps.
	//
	// TODO(sumeer): use the configured restartInterval for the sstable when it
	// was written (which we don't currently store) instead of the default value
	// of 16.
	const nextCmpThresholdBeforeSeek = 3
	const nextFastThresholdBeforeRestarts = 16
	nextCmpCount := 0
	nextFastCount := 0
	usedRestarts := false

	if invariants.Enabled && !i.valid() {
		panic(errors.AssertionFailedf("nextPrefixV3 called on invalid blockIter"))
	}
	prevKeyIsSet := i.ikey.Kind() == InternalKeyKindSet
	for {
		i.offset = i.nextOffset
		if !i.valid() {
			return nil, base.LazyValue{}
		}

		ptr := unsafe.Pointer(uintptr(i.ptr) + uintptr(i.offset))

		// Decode the shared key length integer.
		var shared uint32
		if a := *((*uint8)(ptr)); a < 128 {
			shared = uint32(a)
			ptr = unsafe.Pointer(uintptr(ptr) + 1)
		} else if a, b := a&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 1))); b < 128 {
			shared = uint32(b)<<7 | uint32(a)
			ptr = unsafe.Pointer(uintptr(ptr) + 2)
		} else if b, c := b&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 2))); c < 128 {
			shared = uint32(c)<<14 | uint32(b)<<7 | uint32(a)
			ptr = unsafe.Pointer(uintptr(ptr) + 3)
		} else if c, d := c&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 3))); d < 128 {
			shared = uint32(d)<<21 | uint32(c)<<14 | uint32(b)<<7 | uint32(a)
			ptr = unsafe.Pointer(uintptr(ptr) + 4)
		} else {
			d, e := d&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 4)))
			shared = uint32(e)<<28 | uint32(d)<<21 | uint32(c)<<14 | uint32(b)<<7 | uint32(a)
			ptr = unsafe.Pointer(uintptr(ptr) + 5)
		}
		// Decode the unshared key length integer.
		var unshared uint32
		if a := *((*uint8)(ptr)); a < 128 {
			unshared = uint32(a)
			ptr = unsafe.Pointer(uintptr(ptr) + 1)
		} else if a, b := a&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 1))); b < 128 {
			unshared = uint32(b)<<7 | uint32(a)
			ptr = unsafe.Pointer(uintptr(ptr) + 2)
		} else if b, c := b&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 2))); c < 128 {
			unshared = uint32(c)<<14 | uint32(b)<<7 | uint32(a)
			ptr = unsafe.Pointer(uintptr(ptr) + 3)
		} else if c, d := c&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 3))); d < 128 {
			unshared = uint32(d)<<21 | uint32(c)<<14 | uint32(b)<<7 | uint32(a)
			ptr = unsafe.Pointer(uintptr(ptr) + 4)
		} else {
			d, e := d&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 4)))
			unshared = uint32(e)<<28 | uint32(d)<<21 | uint32(c)<<14 | uint32(b)<<7 | uint32(a)
			ptr = unsafe.Pointer(uintptr(ptr) + 5)
		}
		// Decode the value length integer.
		var value uint32
		if a := *((*uint8)(ptr)); a < 128 {
			value = uint32(a)
			ptr = unsafe.Pointer(uintptr(ptr) + 1)
		} else if a, b := a&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 1))); b < 128 {
			value = uint32(b)<<7 | uint32(a)
			ptr = unsafe.Pointer(uintptr(ptr) + 2)
		} else if b, c := b&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 2))); c < 128 {
			value = uint32(c)<<14 | uint32(b)<<7 | uint32(a)
			ptr = unsafe.Pointer(uintptr(ptr) + 3)
		} else if c, d := c&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 3))); d < 128 {
			value = uint32(d)<<21 | uint32(c)<<14 | uint32(b)<<7 | uint32(a)
			ptr = unsafe.Pointer(uintptr(ptr) + 4)
		} else {
			d, e := d&0x7f, *((*uint8)(unsafe.Pointer(uintptr(ptr) + 4)))
			value = uint32(e)<<28 | uint32(d)<<21 | uint32(c)<<14 | uint32(b)<<7 | uint32(a)
			ptr = unsafe.Pointer(uintptr(ptr) + 5)
		}
		if i.transforms.SyntheticPrefix != nil {
			shared += uint32(len(i.transforms.SyntheticPrefix))
		}

		valuePtr := unsafe.Pointer(uintptr(ptr) + uintptr(unshared))
		i.nextOffset = int32(uintptr(valuePtr)-uintptr(i.ptr)) + int32(value)
		if invariants.Enabled && unshared < 8 {

			panic(errors.AssertionFailedf("unshared %d is too small", unshared))
		}

		keyKind := InternalKeyKind((*[manual.MaxArrayLen]byte)(ptr)[unshared-8])
		keyKind = keyKind & base.InternalKeyKindSSTableInternalObsoleteMask
		prefixChanged := false
		if keyKind == InternalKeyKindSet {
			if invariants.Enabled && value == 0 {
				panic(errors.AssertionFailedf("value is of length 0, but we expect a valuePrefix"))
			}
			valPrefix := *((*valuePrefix)(valuePtr))
			if setHasSamePrefix(valPrefix) {

				nextFastCount++
				if nextFastCount > nextFastThresholdBeforeRestarts {
					if usedRestarts {

						break
					}

					targetOffset := i.offset
					var index int32
					{

						upper := i.numRestarts
						for index < upper {
							h := int32(uint(index+upper) >> 1)

							offset := decodeRestart(i.data[i.restarts+4*h:])
							if offset < targetOffset {
								index = h + 1
							} else {
								upper = h
							}
						}

					}
					usedRestarts = true
					nextFastCount = 0
					if index == i.numRestarts {

						continue
					}

					startingIndex := index
					for index != i.numRestarts &&

						i.data[i.restarts+4*index+3]&restartMaskLittleEndianHighByteOnlySetHasSamePrefix != 0 {

						index++
					}

					if index != startingIndex {

						i.offset = decodeRestart(i.data[i.restarts+4*(index-1):])
						i.streamlinedReadEntry()
					}

					continue
				}
				continue
			} else if prevKeyIsSet {
				prefixChanged = true
			}
		} else {
			prevKeyIsSet = false
		}

		unsharedKey := getBytes(ptr, int(unshared))

		i.fullKey = append(i.fullKey[:shared], unsharedKey...)
		i.val = getBytes(valuePtr, int(value))
		if shared == 0 {

			i.key = unsharedKey
		} else {
			i.key = i.fullKey
		}

		hiddenPoint := false
		if n := len(i.key) - 8; n >= 0 {
			trailer := binary.LittleEndian.Uint64(i.key[n:])
			hiddenPoint = i.transforms.HideObsoletePoints &&
				(trailer&trailerObsoleteBit != 0)
			i.ikey.Trailer = trailer & trailerObsoleteMask
			i.ikey.UserKey = i.key[:n:n]
			if n := i.transforms.SyntheticSeqNum; n != 0 {
				i.ikey.SetSeqNum(uint64(n))
			}
			if i.transforms.SyntheticSuffix != nil {

				prefixLen := i.split(i.ikey.UserKey)
				if cap(i.ikey.UserKey) >= prefixLen+len(i.transforms.SyntheticSuffix) {
					i.ikey.UserKey = append(i.ikey.UserKey[:prefixLen], i.transforms.SyntheticSuffix...)
				} else {
					i.synthSuffixBuf = append(i.synthSuffixBuf[:0], i.ikey.UserKey[:prefixLen]...)
					i.synthSuffixBuf = append(i.synthSuffixBuf, i.transforms.SyntheticSuffix...)
					i.ikey.UserKey = i.synthSuffixBuf
				}
			}
		} else {
			i.ikey.Trailer = uint64(InternalKeyKindInvalid)
			i.ikey.UserKey = nil
		}
		nextCmpCount++
		if invariants.Enabled && prefixChanged && i.cmp(i.ikey.UserKey, succKey) < 0 {
			panic(errors.AssertionFailedf("prefix should have changed but %x < %x",
				i.ikey.UserKey, succKey))
		}
		if prefixChanged || i.cmp(i.ikey.UserKey, succKey) >= 0 {

			if hiddenPoint {
				return i.StreamlinedNext()
			}
			if invariants.Enabled && !i.lazyValueHandling.hasValuePrefix {
				panic(errors.AssertionFailedf("nextPrefixV3 being run for non-v3 sstable"))
			}
			if base.TrailerKind(i.ikey.Trailer) != InternalKeyKindSet {
				i.lazyValue = base.MakeInPlaceValue(i.val)
			} else if i.lazyValueHandling.vbr == nil || !isValueHandle(valuePrefix(i.val[0])) {
				i.lazyValue = base.MakeInPlaceValue(i.val[1:])
			} else {
				i.lazyValue = i.lazyValueHandling.vbr.getLazyValueForPrefixAndValueHandle(i.val)
			}
			return &i.ikey, i.lazyValue
		}

		if nextCmpCount >= nextCmpThresholdBeforeSeek {
			break
		}
	}
	return i.StreamlinedSeekGE(succKey, base.SeekGEFlagsNone)
}

// Prev implements internalIterator.Prev, as documented in the pebble
// package.
func (i *blockIter) StreamlinedPrev() (*InternalKey, base.LazyValue) {
start:
	for n := len(i.cached) - 1; n >= 0; n-- {
		i.nextOffset = i.offset
		e := &i.cached[n]
		i.offset = e.offset
		i.val = getBytes(unsafe.Pointer(uintptr(i.ptr)+uintptr(e.valStart)), int(e.valSize))

		i.key = i.cachedBuf[e.keyStart:e.keyEnd]
		if n := len(i.key) - 8; n >= 0 {
			trailer := binary.LittleEndian.Uint64(i.key[n:])
			hiddenPoint := i.transforms.HideObsoletePoints &&
				(trailer&trailerObsoleteBit != 0)
			if hiddenPoint {
				continue
			}
			i.ikey.Trailer = trailer & trailerObsoleteMask
			i.ikey.UserKey = i.key[:n:n]
			if n := i.transforms.SyntheticSeqNum; n != 0 {
				i.ikey.SetSeqNum(uint64(n))
			}
			if i.transforms.SyntheticSuffix != nil {

				prefixLen := i.split(i.ikey.UserKey)

				i.synthSuffixBuf = append(i.synthSuffixBuf[:0], i.ikey.UserKey[:prefixLen]...)
				i.synthSuffixBuf = append(i.synthSuffixBuf, i.transforms.SyntheticSuffix...)
				i.ikey.UserKey = i.synthSuffixBuf
			}
		} else {
			i.ikey.Trailer = uint64(InternalKeyKindInvalid)
			i.ikey.UserKey = nil
		}
		i.cached = i.cached[:n]
		if !i.lazyValueHandling.hasValuePrefix ||
			base.TrailerKind(i.ikey.Trailer) != InternalKeyKindSet {
			i.lazyValue = base.MakeInPlaceValue(i.val)
		} else if i.lazyValueHandling.vbr == nil || !isValueHandle(valuePrefix(i.val[0])) {
			i.lazyValue = base.MakeInPlaceValue(i.val[1:])
		} else {
			i.lazyValue = i.lazyValueHandling.vbr.getLazyValueForPrefixAndValueHandle(i.val)
		}
		return &i.ikey, i.lazyValue
	}

	i.clearCache()
	if i.offset <= 0 {
		i.offset = -1
		i.nextOffset = 0
		return nil, base.LazyValue{}
	}

	targetOffset := i.offset
	var index int32

	{

		upper := i.numRestarts
		for index < upper {
			h := int32(uint(index+upper) >> 1)

			offset := decodeRestart(i.data[i.restarts+4*h:])
			if offset < targetOffset {

				index = h + 1
			} else {
				upper = h
			}
		}

	}

	i.offset = 0
	if index > 0 {
		i.offset = decodeRestart(i.data[i.restarts+4*(index-1):])
	}

	i.streamlinedReadEntry()

	for i.nextOffset < targetOffset {
		i.cacheEntry()
		i.offset = i.nextOffset
		i.streamlinedReadEntry()
	}

	hiddenPoint := i.decodeInternalKey(i.key)
	if hiddenPoint {

		goto start
	}
	if i.transforms.SyntheticSuffix != nil {

		prefixLen := i.split(i.ikey.UserKey)

		i.synthSuffixBuf = append(i.synthSuffixBuf[:0], i.ikey.UserKey[:prefixLen]...)
		i.synthSuffixBuf = append(i.synthSuffixBuf, i.transforms.SyntheticSuffix...)
		i.ikey.UserKey = i.synthSuffixBuf
	}
	if !i.lazyValueHandling.hasValuePrefix ||
		base.TrailerKind(i.ikey.Trailer) != InternalKeyKindSet {
		i.lazyValue = base.MakeInPlaceValue(i.val)
	} else if i.lazyValueHandling.vbr == nil || !isValueHandle(valuePrefix(i.val[0])) {
		i.lazyValue = base.MakeInPlaceValue(i.val[1:])
	} else {
		i.lazyValue = i.lazyValueHandling.vbr.getLazyValueForPrefixAndValueHandle(i.val)
	}
	return &i.ikey, i.lazyValue
}
